/**

\page tutorial-tracking-rbt Tutorial: Tracking with the Render-Based Tracker
\tableofcontents

\section rbt_tracking_intro Introduction

In ViSP 3.7, we are introducing a new Render-Based Tracker (RBT), that leverages rendering on the GPU to extract geometric information
in order to perform online and continuous 6D pose estimation of a known 3D object.

It is an extension of the Model-Based Tracker already present in ViSP (see \ref tutorial-tracking-mb-generic)
This implementation is derived from the work of \cite Petit14a.

A major advantage over the Model-Based Tracker is that the RBT uses a common mesh representation for the object to track.
This representation does not require additional preprocessing for feature selection and is widely supported in software such as Blender.
It can also be obtained through photogrammetry processes, minimizing the work required before using the tracker.
Note that we do not consider deformable objects, only rigid objects.

Similar to the MBT, tracking is framed as an optimization problem, where the difference between the features (geometric or photometric) extracted from a 3D model and those observed in the image
must be minimized.

the RBT is built as a modular pipeline (\ref rbt_tracking_overview). In practice, this means:

- It can leverage both RGB and depth information (\ref rbt_tracking_requirements)
- It features a complete configuration for every part of the tracking pipeline (\ref rbt_tracking_config)
- Most components can be disabled or adapted to different use cases and scenarios
- Components can be extended: You can add new features or filtering methods. (\ref rbt_extension)



\section rbt_tracking_requirements Requirements

\subsection rbt_tracking_install_requirements Building the RBT module

To successfully build the RBT module, you will need:

- To compile using a minimum standard version of C++11 (defined in the VISP_CXX_STANDARD CMake variable)
- The Panda3D 3rd party, which is used for 3D rendering. See \ref tutorial-panda3d-install for more information on how to install Panda3D on your system.
Panda3D is also available through Conda if you are compiling in a virtual environment

Additionally, Some optional dependencies are (strongly) recommended:
- `nlohmann::json` (\ref install_ubuntu_3rdparty_other) to load configuration files and save your tracking results.
Without it, the tracker setup will have to be done through code
- OpenCV, if you wish to use the `vpRBKltTracker`, that uses KLT feature tracking for pose estimation.

\subsection rbt_tracking_requirements General requirements

To use the RBT you will need several things:

- An OpenGL or DirectX enabled device, that can perform 3D rendering.
You can use software acceleration, although having a GPU is preferable for performance reasons
- A camera with known intrinsic parameters. For calibration see \ref tutorial-calibration-intrinsic. \important The camera intrinsics should follow a model without distortion.
- If you are using an RGB-D camera, the depth image should be aligned with the RGB image. Some SDKs provide this functionality.
For instance, the wrapper around the realsense SDK accepts an "align" parameter in the vpRealSense2::acquire function.
When correctly set, the alignment will automatically be performed for you.
- A 3D model of the object, in a format that is supported by Panda3D (See \ref tutorial-panda3d-file-conversion ).
If you have installed *libassimp-dev* or are using the conda package, Panda3D supports common formats such as *.ply*, *.obj*. Otherwise, you will have to convert your mesh using the previously linked method.


\subsection rbt_tracking_requirements_3d_model 3D model considerations

There are very little restrictions on the 3D model that can be used in the RBT.

First, your 3D model's size should be expressed in meters. Be aware that some CAD software export models in millimeters.

If you are using the initialization by click, you will have to be careful with the model orientation when exporting (see below).

Note that while 3D meshes support textures, the presently available RBT features do not use the texture information.

To correctly process your model, here is a small overview of the steps to follow in Blender

\subsubsection rbt_tracking_requirements_model_preparation Preparing your model in blender

To make sure your model is correct, you should start by setting the scale and orientation of your 3D model.

You should first import your model in Blender. In the top left corner of the Blender window, click on **File > Import > (Your model type)**, then pick your object in the popup window.

Your model should have been imported and should be visible in the 3D viewer. You should then click on it and press **N** to bring up the transform panel in the top right corner.
Then you should press **Ctrl+A** to open the *Apply* menu and click on the "**All transforms**" item. This ensures that the exported model matches what is seen in Blender.

\image html rbt-blender-apply-transforms.png Apply transforms to clear any rotation and scale difference stored in Blender.



If you are using initialization by click as described in \ref mb_generic_init_user, you can select the 3D points to click directly in Blender.
To do so, first click on your object and press *tab* to go into Edit mode. Then, press *N* to display the **Transform** Panel (Top right of the viewer).
Choose a 3D point to use for initilization, then click on it. In the **Transform** panel, you should then see the XYZ coordinates of the point, which you can copy into the init file.
Be sure to select the "Global" frame in the transform panel.

\image html rbt-click-init-blender-panel.png The transform panel with a selected 3D point in Edit mode.

Once you have selected your points make sure to export your model in the same frame as the Blender frame.
You can export your 3D model, using the **File > Export > (Supported 3D format such as .obj)** and save your model in the desired location.
In the export panel, set "Forward Axis" to "**-Z**" and "Up Axis" to "**Y**"



With all the prerequisites met, you can now understand (if you wish) how the tracker works and how to customize it for your needs.

\section rbt_tracking_overview Algorithm overview

This section of the tutorial details how the tracker works.

The RBT is used by building a vpRBTracker object, configuring it and then feeding it RGB or RGB-D frames.

As the RBT is a tracking algorithm, an initial pose must be provided. Given an initial pose, the RBT will continuously track an object in a sequence of frames.
This also assumes that the motion between consecutive frame is small enough so that the tracker can retrieve the features and correctly update the pose of the object in the camera frame.
Otherwise, there is a risk of divergence, leading to a need for reinitialization.
This can be done via click (see \ref mb_generic_init_user and the vpRBTracker::initClick method).
Otherwise, you can use a pose estimation algorithm (e.g. Megapose \ref tutorial-tracking-megapose and vpMegaPose::estimatePoses) followed by a call to vpRBTracker::setPose.


Roughly the algorithm works as follows:

1. Given an initial pose of the object in the camera frame \f$^{c}\mathbf{T}_{o}\f$, generate a render of the object at this pose.
2. Extract and process render data, and store it into a vpRBRenderData object.
3. From the renders, seek features to match with in the current RGB and depth images.
4. From the match between image and model features, formulate an error \f$\mathbf{e}\f$.
5. Iteratively minimize \f$\vert\mathbf{e}\vert_2\f$ by updating \f$^{c}\mathbf{T}_{o}\f$


\subsection rbt_tracking_rendering Rendering

Object rendering is done through Panda3D. The object is rendered at the last computed pose.

All the information derived from the render is stored into a vpRBRenderData, in the current frame data (vpRBFeatureTrackerInput).
This object contains information such as:

- the 3D surface normals, expressed as 3D unit vectors
- The depth map, in meters (Rendered using vpPanda3DGeometryRenderer)
- The object silhouette (extracted using vpPanda3DDepthCannyFilter and vpRBTracker::extractSilhouettePoints)
- The image area containing the object (represented by a vpRect)
- the pose at which the object was rendered: When implementing new features or algorithms,
 it is preferable to use this pose when extracting data from the render, instead of those provided in the functions to redefine.

\image html rbt-rendering.png Rendering is used to extract geometric information width=50%


For more information on how rendering is performed you can examine the following sources:
- vpRBTracker::updateRender
- vpRBRenderData
- vpObjectCentricRenderer
- vpPanda3DRendererSet
- vpPanda3DBaseRenderer


\subsection rbt_tracking_features Trackable features

This section details the different features that can be used to track the object.
The RBT is flexible, and each feature can be added or removed, depending on your scenario and available sensor.

For each feature, a weighting function is computed at every iteration, and a user specified weight can be used to consider certain features as more or less important than others.

All features inherit from a base class, vpRBFeatureTracker, that defines the different functionalities that should be implemented for a feature to be trackable.

To add or interact with the features of the RBT, you can use vpRBTracker::addTracker and vpRBTracker::getFeatureTrackers.

\subsubsection rbt_tracking_available_features Feature list

| Feature | Class   | Required inputs | Note
| :----  | :----  | :----:          | ----:
| KLT 2D points     | vpRBKltTracker     | Grayscale image | Requires OpenCV
| Silhouette moving edges  | vpRBSilhouetteMeTracker   | Grayscale image | -
| Silhouette color edges  | vpRBSilhouetteCCDTracker   | Color image  | More expensive than moving edge based tracker
| Depth map (point-to-plane)  | vpRBDenseDepthTracker   | Depth image  | -


- **KLT points** rely on the vpKltOpenCV class to extract and track 2D KLT points from the current image, which are then associated with points on the 3D model.
The error to minimize is then the reprojection error between the 2D points in the current image and the reprojection of their associated 3D points.

- **Silhouette moving edges** Rely on the rendering to first extract the object depth disparities, which are hypothesized to create disparities in the luminance image as well.
For each point of the depth contour, the most likely edge in its neighbourhood in the luminance map is detected using the moving edge framework.

- **Silhouette Color edges** Similarly to the silhouette moving edges, the depth disparity is first used to extract the outer silhouette contours.
Then, based on the Contracting Curve Density algorithm, color statistics are computed inside and outside the silhouette and a per pixel error is computed.
This error is minimized when the difference between the inner and outer color distributions are maximized (i.e, there is a clear difference between the object's color and the environment).
Unlike the moving edge approach, the error is not geometric in nature, but rather photometric.

- **Depth** The depth information can be used and compared to the rendered depth. With this feature, the error to minimize is the point-to-plane distance.
The point is sampled from the current depth map, while the plane is computed from the rendered 3D model, using its distance to the camera and surface normal.
The plane is continuously updated to minmize the distance to the sample depth point.

To define you own features (advanced), see \ref rbt_extension_features.

\subsection rbt_tracking_filtering Feature filtering

When tracking, it is possible for occlusions or lighting artifacts to appear. In thoses cases, considering ill-defined or ill-matched features may lead to tracking failure.
To handle those cases and improve tracking reliability, the RBT comes with two mechanisms:

- A masking/segmentation approach, that outputs a per-pixel probability map that a given pixel belongs to the object
- Robust error criterias. Each feature defined above uses an M-Estimator (through vpRobust) to reweigh the errors to be minimized and reduce the influence of outliers.

While M-estimators are always used, the masking approach is entirely optional.
To enable masking, you should call vpRBTracker::setObjectSegmentationMethod with the method that you wish to use. To disable it, pass it *nullptr* as input argument.
As of now, the only method available is vpColorHistogramMask.
This method computes time-smoothed histograms of the object and background's color distributions,
and compares their output probabilities for a given pixel color to compute an object membership score.
If available, this method may use camera depth information to update the object histograms using only reliable pixels in terms of depth (where object and render are closely aligned)
This method is ideal when there is a clear photometric distinction between object and environment.

You may define your own approach to object segmentation (by e.g., using a neural network to compare feature maps or using more depth information)
by inheriting from vpRBObjectSegmentationMethod.

Note that the use of the probability map output by the segmentation method is implemented differently for each type of feature.
For instance, contour based method will search for strong probability differences along the contour normal,
while point based feature may only look in a small neighbourhood to see if a given area belong to the object.

The diagram below sums up the combination of the masking and outlier rejection steps.

\image html rbt-filtering.jpg Masking is first used to remove features in cases such as strong occlusions. Remaining outliers are filtered through the use of robust estimators. width=25%

\subsection rbt_tracking_optimization Minimization problem


\subsection rbt_drift_detection Detecting tracking failure

\section rbt_tracking_config Configuration

\section rbt_tracking_usage Tracker usage

We will now focus on an example usage of the tracker, using a Realsense camera. While we focus on this example, another one that uses prererecorded sequences is available in \ref rbt_tracking_examples

To run the program, start by placing yourself in the ViSP build directory.
Assuming you have compiled and built the tutorials, navigate to the tutorial folder:
\code{.sh}
~/visp_build $ cd tutorial/tracking/render-based
~/visp_build/tutorial-tracking/render-based $ cd tutorial/tracking/render-based
\endcode

You can then examine the command line arguments with:

\code{.sh}
./tutorial-rbt-realsense  -h
\endcode

Which outputs:
\verbatim
Program description: Tutorial showing the usage of the Render-Based tracker with a RealSense camera
Arguments:
	--config               Path to the JSON configuration file. Values in this files are loaded, and can be overridden by command line arguments.
	                       Optional

	--debug-display        Enable additional displays from the renderer
	                       Default: false

	--fps                  Realsense requested framerate
	                       Default: 60
	                       Optional

	--height               Realsense requested image height
	                       Default: 480
	                       Optional

	--init-file            Path to the JSON file containing the 2D/3D correspondences for initialization by click
	                       Default: ""
	                       Optional

	--max-depth-display    Maximum depth value, used to scale the depth display
	                       Default: 1.0
	                       Optional

	--no-display           Disable display windows
	                       Default: true

	--object               Name of the object to track. Used to potentially fetch the init file
	                       Default: ""
	                       Optional

	--plot-cov             Plot the pose covariance trace for each feature
	                       Default: false

	--plot-divergence      Plot the metrics associated to the divergence threshold computation
	                       Default: false

	--plot-pose            Plot the pose of the object in the camera frame
	                       Default: false

	--plot-position        Plot the position of the object in a 3d figure
	                       Default: false

	--pose                 Initial pose of the object in the camera frame.
	                       Default: []
	                       Optional

	--profile              Enable the use of Pstats to profile rendering times
	                       Default: false

	--save                 Whether to save experiment data
	                       Default: false

	--save-path            Where to save the experiment log. The folder should not exist.
	                       Default: ""
	                       Optional

	--save-video           Whether to save the video
	                       Default: false

	--tracker              Path to the JSON file containing the tracker
	                       Default: ""
	                       Optional

	--video-framerate      Output video framerate
	                       Default: 30
	                       Optional

	--width                Realsense requested image width
	                       Default: 848
	                       Optional

Example JSON configuration file:

{
  "--debug-display": false,
  "--fps": 60,
  "--height": 480,
  "--init-file": "",
  "--max-depth-display": 1.0,
  "--no-display": true,
  "--object": "",
  "--plot-cov": false,
  "--plot-divergence": false,
  "--plot-pose": false,
  "--plot-position": false,
  "--pose": [],
  "--profile": false,
  "--save": false,
  "--save-path": "",
  "--save-video": false,
  "--tracker": "",
  "--video-framerate": 30,
  "--width": 848
}
\endverbatim

You can see that among those arguments, the script has two main parameters.
The first, `--tracker` is the path to the .json configuration file, defined following the details given in \ref rbt_tracking_config.
The second `--object`, is the path to the 3D model (in a format readable by the Panda3D engine). Alternatively, it can specified in the .json file.




\snippet tutorial-rbt-realsense.cpp Command line parsing


\section rbt_tracking_examples Full tutorial examples


The full code for the realsense based tutorial:
\include tutorial-rbt-realsense.cpp


The full code to track an offline, preregistered sequence:
\include tutorial-rbt-sequence.cpp

Parsing and display utils:
\include render-based-tutorial-utils.h


\section rbt_extension Extending the RBT

\subsection rbt_extension_features Defining your own features

\subsection rbt_extension_factory_pattern Registering your own component for JSON parsing

As seen in \ref rbt_tracking_config, the different components of the RBT pipeline can be parsed from a JSON configuration file. This requires a ViSP version that is built with the nlohmann::json third party (see \ref rbt_tracking_install_requirements)

This functionality is implemented using a factory pattern as defined in the vpDynamicFactory.

For your own class to be loadable, you should call vpDynamicFactory::registerType with a lambda function that defines how a JSON object can be used to configure an object from your type

For instance, in the vpRBDriftDetectorFactory, an object of type vpRBProbabilistic3DDriftDetector is registered as:
\code{.cpp}
registerType("probabilistic", [](const nlohmann::json &j) {
  std::shared_ptr<vpRBProbabilistic3DDriftDetector> p(new vpRBProbabilistic3DDriftDetector());
  p->loadJsonConfiguration(j);
  return p;
});
\endcode

This means that when parsing the JSON object's drift field, if the **"type"** field is equal to "probabilistic", then this function will be called and the json object will used to build a vpRBProbabilistic3DDriftDetector.

This patterns hold for the different types of components:

- vpRBFeatureTrackerFactory to register subtypes of vpRBFeatureTracker
- vpRBDriftDetectorFactory to register vpRBDriftDetector
- vpObjectMaskFactory to register vpObjectMask




*/
